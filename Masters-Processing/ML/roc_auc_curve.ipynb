{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from mrmr_wrapper import MRMRTransformer\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score, confusion_matrix, \n",
    "                            roc_curve, precision_recall_curve, \n",
    "                            average_precision_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aff1dac",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaca0175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"features-Master.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_shuffled = shuffled.iloc[:, 4:]\n",
    "labels_shuffled = shuffled[\"Comfort Score\"]\n",
    "\n",
    "print(\"Number of Comfort Score == 3:\", (labels_shuffled == 3).sum())\n",
    "\n",
    "# Create binary labels (1,2 = 0; 4,5 = 1; exclude 3 for clearer separation)\n",
    "binary_labels = labels_shuffled.apply(lambda x: 0 if x <= 2 else (1 if x >=4 else np.nan))\n",
    "binary_data = data_shuffled[~binary_labels.isna()]\n",
    "binary_labels = binary_labels[~binary_labels.isna()] # The \"~\" operator is used to filter out NaN values (i.e., rows where Comfort Score == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915a7bc",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cb9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For binary classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    binary_data,\n",
    "    binary_labels,\n",
    "    test_size=0.2,\n",
    "    stratify=binary_labels,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1130f9",
   "metadata": {},
   "source": [
    "# Run optimization for feature selection and Catboost classification parameters for a variable number of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trial numbers\n",
    "trial_numbers = [5, 10, 25, 50, 100, 200, 250, 300, 400, 500]\n",
    "\n",
    "# Store results\n",
    "results = {\n",
    "    'n_trials': [],\n",
    "    'best_roc_auc_cv': [],  # Cross-validation ROC-AUC\n",
    "    'test_roc_auc': [],     # Test set ROC-AUC\n",
    "    'test_accuracy': [],\n",
    "    'test_precision': [],\n",
    "    'test_recall': [],\n",
    "    'test_f1': [],\n",
    "    'best_params': []\n",
    "}\n",
    "\n",
    "X = X_train.copy()\n",
    "y = y_train.copy()\n",
    "\n",
    "def binary_classification_objective(trial):\n",
    "    # Feature selection\n",
    "    fs_method = trial.suggest_categorical('feature_selection', ['MRMR', 'RFE', 'None'])\n",
    "    \n",
    "    if fs_method != 'None':\n",
    "        k_features = trial.suggest_int('k_features', 5, 105, step=10)\n",
    "        if fs_method == 'RFE':\n",
    "            estimator = RandomForestClassifier(random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=k_features)\n",
    "        else: # MRMR\n",
    "            selector = MRMRTransformer(k_features=k_features)\n",
    "    else:\n",
    "        selector = 'passthrough'\n",
    "    \n",
    "    # CatBoost hyperparameters\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000, step=25),\n",
    "        'depth': trial.suggest_int('depth', 6, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True), \n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-9, 10, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_seed': 42,\n",
    "        'verbose': False,\n",
    "    }\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('feature_selection', selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    try:\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='roc_auc', n_jobs=1)\n",
    "        return np.mean(scores)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in trial: {e}\")\n",
    "        return -np.inf\n",
    "\n",
    "def evaluate_on_test_set(best_params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the best model from optimization on test set\n",
    "    \"\"\"\n",
    "    # Apply feature selection to training data\n",
    "    best_fs_method = best_params.get('feature_selection', 'None')\n",
    "    \n",
    "    if best_fs_method != 'None':\n",
    "        k_features = best_params['k_features']\n",
    "        if best_fs_method == 'RFE':\n",
    "            estimator = RandomForestClassifier(random_state=42)\n",
    "            selector = RFE(estimator, n_features_to_select=k_features)\n",
    "        elif best_fs_method == 'MRMR':\n",
    "            selector = MRMRTransformer(k_features=k_features)\n",
    "        \n",
    "        selector.fit(X_train, y_train)\n",
    "        if hasattr(selector, 'get_support'):  # For RFE\n",
    "            selected_features = X_train.columns[selector.get_support()]\n",
    "        else:  # For MRMRTransformer\n",
    "            selected_features = selector.selected_features\n",
    "        X_train_best = X_train[selected_features]\n",
    "    else:\n",
    "        X_train_best = X_train\n",
    "        selected_features = X_train.columns\n",
    "\n",
    "    # Apply the same feature selection to test data\n",
    "    if best_fs_method != 'None':\n",
    "        if best_fs_method == 'MRMR':\n",
    "            X_test_best = X_test[selected_features]\n",
    "        else:\n",
    "            X_test_best = selector.transform(X_test)\n",
    "            if isinstance(X_test, pd.DataFrame):\n",
    "                X_test_best = pd.DataFrame(X_test_best, columns=selected_features, index=X_test.index)\n",
    "    else:\n",
    "        X_test_best = X_test\n",
    "\n",
    "    # Create and train final model\n",
    "    final_model = CatBoostClassifier(\n",
    "        iterations=best_params['iterations'],\n",
    "        depth=best_params['depth'],\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        l2_leaf_reg=best_params['l2_leaf_reg'],\n",
    "        random_strength=best_params['random_strength'],\n",
    "        bagging_temperature=best_params['bagging_temperature'],\n",
    "        border_count=best_params['border_count'],\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Train on full training data\n",
    "    final_model.fit(X_train_best, y_train)\n",
    "\n",
    "    # Make predictions on test set\n",
    "    y_pred = final_model.predict(X_test_best)\n",
    "    y_pred_proba = final_model.predict_proba(X_test_best)[:, 1]\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'selected_features': selected_features if best_fs_method != 'None' else 'All features'\n",
    "    }\n",
    "\n",
    "# Run optimization for each number of trials\n",
    "for n_trials in trial_numbers:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running {n_trials} trials and evaluating on test set...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create study with TPESampler\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        binary_classification_objective, \n",
    "        n_trials=n_trials, \n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_results = evaluate_on_test_set(\n",
    "        study.best_params, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        X_test, \n",
    "        y_test\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results['n_trials'].append(n_trials)\n",
    "    results['best_roc_auc_cv'].append(study.best_value) # Cross-validation ROC-AUC on training set\n",
    "    results['test_roc_auc'].append(test_results['roc_auc'])\n",
    "    results['test_accuracy'].append(test_results['accuracy'])\n",
    "    results['test_precision'].append(test_results['precision'])\n",
    "    results['test_recall'].append(test_results['recall'])\n",
    "    results['test_f1'].append(test_results['f1'])\n",
    "    results['best_params'].append(study.best_params)\n",
    "    \n",
    "    print(f\"CV ROC-AUC after {n_trials} trials: {study.best_value:.4f}\")\n",
    "    print(f\"Test ROC-AUC after {n_trials} trials: {test_results['roc_auc']:.4f}\")\n",
    "    print(f\"Feature selection: {test_results['selected_features']}\")\n",
    "\n",
    "# Plot the ROC-AUC curves\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Subplot 1: ROC-AUC comparison\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(results['n_trials'], results['best_roc_auc_cv'], 'bo-', linewidth=2, markersize=8, label='Train ROC AUC')\n",
    "plt.plot(results['n_trials'], results['test_roc_auc'], 'ro-', linewidth=2, markersize=8, label='Test ROC AUC')\n",
    "plt.xlabel('Number of Trials', fontsize=12)\n",
    "plt.ylabel('ROC AUC Score [Train & Test]', fontsize=12)\n",
    "plt.title('Optimization Progress: ROC AUC vs Number of Trials', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(results['n_trials'])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "# Add value annotations\n",
    "for i, (n, cv_score, test_score) in enumerate(zip(results['n_trials'], results['best_roc_auc_cv'], results['test_roc_auc'])):\n",
    "    plt.annotate(f'Train: {cv_score:.3f}\\nTest: {test_score:.3f}', \n",
    "                (n, max(cv_score, test_score)), \n",
    "                textcoords=\"offset points\", \n",
    "                xytext=(0,15), ha='center', fontsize=8)\n",
    "\n",
    "# Subplot 2: Other metrics\n",
    "plt.subplot(2, 1, 2)\n",
    "metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']\n",
    "labels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "colors = ['green', 'orange', 'purple', 'brown']\n",
    "\n",
    "for metric, label, color in zip(metrics, labels, colors):\n",
    "    plt.plot(results['n_trials'], results[metric], 'o-', linewidth=2, markersize=6, label=label, color=color)\n",
    "\n",
    "plt.xlabel('Number of Trials', fontsize=12)\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.title('Test Set Metrics vs Number of Trials', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(results['n_trials'])\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL OPTIMIZATION AND TEST SET RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Trials':>8} {'Train ROC AUC':>10} {'Test ROC AUC':>12} {'Accuracy':>10} {'Precision':>10} {'Recall':>8} {'F1':>8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, n_trials in enumerate(results['n_trials']):\n",
    "    print(f\"{n_trials:8d} {results['best_roc_auc_cv'][i]:10.4f} {results['test_roc_auc'][i]:12.4f} \"\n",
    "          f\"{results['test_accuracy'][i]:10.4f} {results['test_precision'][i]:10.4f} \"\n",
    "          f\"{results['test_recall'][i]:8.4f} {results['test_f1'][i]:8.4f}\")\n",
    "\n",
    "# Show best overall result\n",
    "best_overall_idx = np.argmax(results['test_roc_auc'])\n",
    "print(f\"\\nBest overall test result:\")\n",
    "print(f\"Trials: {results['n_trials'][best_overall_idx]} | Test ROC AUC: {results['test_roc_auc'][best_overall_idx]:.4f}\")\n",
    "print(\"Best parameters:\")\n",
    "for key, value in results['best_params'][best_overall_idx].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bessy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
