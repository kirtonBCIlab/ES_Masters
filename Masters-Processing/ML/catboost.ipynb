{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0855e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor,\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import mrmr\n",
    "from mrmr import mrmr_classif, mrmr_regression\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ca7485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"features-Master.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_shuffled = shuffled.iloc[:, 4:]\n",
    "labels_shuffled = shuffled[\"Comfort Score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6032c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRMRTransformer:\n",
    "    def __init__(self, k_features):\n",
    "        self.k_features = k_features\n",
    "        self.selected_features = None\n",
    "        self.column_names = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Convert to DataFrame if not already\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        # Reset indices to avoid alignment issues\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "        \n",
    "        self.column_names = X.columns.tolist()\n",
    "        try:\n",
    "            self.selected_features = mrmr_regression(X, y, K=self.k_features)\n",
    "        except:\n",
    "            # Fallback to random features if MRMR fails\n",
    "            self.selected_features = np.random.choice(X.columns, size=min(self.k_features, len(X.columns)), replace=False)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.column_names)\n",
    "        return X[self.selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddd4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression (using stratified split based on binned target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_shuffled, \n",
    "    labels_shuffled, \n",
    "    test_size=0.2, \n",
    "    stratify=labels_shuffled,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd88aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c44c1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:10:30,562] A new study created in memory with name: no-name-bdf09108-c9a8-433c-97b5-09d5c6556eec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc0538a36e24c4c8d1224f5ae5b842c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:01<00:00, 29.97it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 29.46it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 29.85it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 30.48it/s]\n",
      "100%|██████████| 36/36 [00:01<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:10:45,501] Trial 0 finished with value: 0.11565104562749388 and parameters: {'imputation': 'knn', 'knn_neighbors': 13, 'feature_selection': 'MRMR', 'k_features': 36, 'iterations': 271, 'depth': 6, 'learning_rate': 0.05578236269529149, 'l2_leaf_reg': 3.3457696232145562e-06}. Best is trial 0 with value: 0.11565104562749388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 30.85it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 30.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 30.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 28.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 29.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:11:47,120] Trial 1 finished with value: 0.10966740068679467 and parameters: {'imputation': 'knn', 'knn_neighbors': 15, 'feature_selection': 'MRMR', 'k_features': 10, 'iterations': 569, 'depth': 10, 'learning_rate': 0.0032751792662942896, 'l2_leaf_reg': 2.8355325138293217e-05}. Best is trial 0 with value: 0.11565104562749388.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 29.18it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 29.87it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 30.30it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 30.37it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:12:02,426] Trial 2 finished with value: 0.09176786044978617 and parameters: {'imputation': 'median', 'feature_selection': 'MRMR', 'k_features': 32, 'iterations': 480, 'depth': 7, 'learning_rate': 0.0679241145699015, 'l2_leaf_reg': 2.5670209614291212e-05}. Best is trial 0 with value: 0.11565104562749388.\n",
      "[I 2025-08-12 14:12:39,526] Trial 3 finished with value: 0.13840401446586686 and parameters: {'imputation': 'median', 'feature_selection': 'None', 'iterations': 655, 'depth': 5, 'learning_rate': 0.0015687012265668768, 'l2_leaf_reg': 0.10621960212845766}. Best is trial 3 with value: 0.13840401446586686.\n",
      "[I 2025-08-12 14:12:42,868] Trial 4 finished with value: 0.1169935492416047 and parameters: {'imputation': 'median', 'feature_selection': 'ANOVA', 'k_features': 27, 'iterations': 214, 'depth': 7, 'learning_rate': 0.03851569715487032, 'l2_leaf_reg': 0.0019521343673036002}. Best is trial 3 with value: 0.13840401446586686.\n",
      "[I 2025-08-12 14:30:30,818] Trial 5 finished with value: 0.17722653375964556 and parameters: {'imputation': 'iterative', 'iterative_max_iter': 35, 'feature_selection': 'MutualInfo', 'k_features': 24, 'iterations': 115, 'depth': 6, 'learning_rate': 0.04139443959238489, 'l2_leaf_reg': 0.00016578022967780138}. Best is trial 5 with value: 0.17722653375964556.\n",
      "[I 2025-08-12 14:36:50,780] Trial 6 finished with value: 0.19885061826426298 and parameters: {'imputation': 'median', 'feature_selection': 'MutualInfo', 'k_features': 46, 'iterations': 897, 'depth': 10, 'learning_rate': 0.009481437902555673, 'l2_leaf_reg': 1.0724187913941066}. Best is trial 6 with value: 0.19885061826426298.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 26.25it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 28.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 28.05it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 28.69it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-12 14:58:17,835] Trial 7 finished with value: 0.08169783273458284 and parameters: {'imputation': 'iterative', 'iterative_max_iter': 39, 'feature_selection': 'MRMR', 'k_features': 16, 'iterations': 392, 'depth': 4, 'learning_rate': 0.02854855169240176, 'l2_leaf_reg': 1.5657512019387056e-07}. Best is trial 6 with value: 0.19885061826426298.\n",
      "[I 2025-08-12 15:10:19,858] Trial 8 finished with value: 0.08328169190284493 and parameters: {'imputation': 'median', 'feature_selection': 'None', 'iterations': 653, 'depth': 9, 'learning_rate': 0.0015392368323680547, 'l2_leaf_reg': 2.806756879192612}. Best is trial 6 with value: 0.19885061826426298.\n",
      "[W 2025-08-12 15:10:19,862] Trial 9 failed with parameters: {'imputation': 'median', 'feature_selection': 'RFE', 'k_features': 23, 'rfe_step': 0.7532741894493715} because of the following error: NameError(\"name 'RandomForestRegressor' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_30652\\1942122626.py\", line 31, in objective\n",
      "    estimator = RandomForestRegressor(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "NameError: name 'RandomForestRegressor' is not defined\n",
      "[W 2025-08-12 15:10:19,909] Trial 9 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Run Optuna Study\u001b[39;00m\n\u001b[32m     69\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Best result\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\bessy\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fs_method == \u001b[33m'\u001b[39m\u001b[33mRFE\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     30\u001b[39m     rfe_step = trial.suggest_float(\u001b[33m'\u001b[39m\u001b[33mrfe_step\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.1\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     estimator = \u001b[43mRandomForestRegressor\u001b[49m(\n\u001b[32m     32\u001b[39m         n_estimators=trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mrfe_n_estimators\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m50\u001b[39m, \u001b[32m200\u001b[39m),\n\u001b[32m     33\u001b[39m         max_depth=trial.suggest_int(\u001b[33m'\u001b[39m\u001b[33mrfe_max_depth\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m10\u001b[39m),\n\u001b[32m     34\u001b[39m         random_state=\u001b[32m42\u001b[39m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     selector = RFE(estimator, n_features_to_select=k_features, step=rfe_step)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fs_method == \u001b[33m'\u001b[39m\u001b[33mMRMR\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[31mNameError\u001b[39m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# X_train, y_train should be defined beforehand\n",
    "X = X_train_scaled.copy()\n",
    "y = y_train.copy().values  # Ensure numpy array for y\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Imputation\n",
    "    impute_method = trial.suggest_categorical('imputation', ['mean', 'median', 'knn', 'iterative'])\n",
    "    if impute_method == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    elif impute_method == 'median':\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    elif impute_method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=trial.suggest_int('knn_neighbors', 3, 15))\n",
    "    else:\n",
    "        imputer = IterativeImputer(\n",
    "            max_iter=trial.suggest_int('iterative_max_iter', 10, 50),\n",
    "            random_state=42,\n",
    "            tol=0.01\n",
    "        )\n",
    "\n",
    "    # 2. Feature Selection\n",
    "    fs_method = trial.suggest_categorical('feature_selection', ['ANOVA', 'MutualInfo', 'RFE', 'MRMR', 'None'])\n",
    "    if fs_method != 'None':\n",
    "        k_features = trial.suggest_int('k_features', 10, min(50, X.shape[1]))\n",
    "        if fs_method == 'ANOVA':\n",
    "            selector = SelectKBest(f_regression, k=k_features)\n",
    "        elif fs_method == 'MutualInfo':\n",
    "            selector = SelectKBest(mutual_info_regression, k=k_features)\n",
    "        elif fs_method == 'RFE':\n",
    "            rfe_step = trial.suggest_float('rfe_step', 0.1, 1.0)\n",
    "            estimator = RandomForestRegressor(\n",
    "                n_estimators=trial.suggest_int('rfe_n_estimators', 50, 200),\n",
    "                max_depth=trial.suggest_int('rfe_max_depth', 3, 10),\n",
    "                random_state=42\n",
    "            )\n",
    "            selector = RFE(estimator, n_features_to_select=k_features, step=rfe_step)\n",
    "        elif fs_method == 'MRMR':\n",
    "            selector = MRMRTransformer(k_features=k_features)\n",
    "    else:\n",
    "        selector = 'passthrough'\n",
    "\n",
    "    # 3. CatBoost Parameters\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'verbose': False\n",
    "    }\n",
    "    model = CatBoostRegressor(**params)\n",
    "\n",
    "    # 4. Pipeline \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('feature_selection', selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # 5. Cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2', n_jobs=1)\n",
    "        return np.mean(scores)\n",
    "    except Exception:\n",
    "        return -np.inf\n",
    "\n",
    "# Run Optuna Study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"R²: {trial.value:.4f}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# 1. Parallel Coordinate Plot (Best for seeing parameter relationships)\n",
    "fig = vis.plot_parallel_coordinate(\n",
    "    study,\n",
    "    params=[\n",
    "        'imputation',\n",
    "        'feature_selection',\n",
    "        'k_features',\n",
    "        'model',\n",
    "        'n_estimators',\n",
    "        'max_depth',\n",
    "        'learning_rate'\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 2. Parameter Importance Plot\n",
    "fig = vis.plot_param_importances(study, target_name=\"R² Score\")\n",
    "fig.show()\n",
    "\n",
    "# 3. Slice Plot (Best for seeing individual parameter effects)\n",
    "fig = vis.plot_slice(\n",
    "    study,\n",
    "    params=[\n",
    "        'n_estimators',\n",
    "        'max_depth',\n",
    "        'learning_rate',\n",
    "        'k_features'\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4. Contour Plot (For seeing parameter interactions)\n",
    "fig = vis.plot_contour(\n",
    "    study,\n",
    "    params=[\n",
    "        ('n_estimators', 'max_depth'),\n",
    "        ('learning_rate', 'max_depth'),\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193973de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best imputation method from study\n",
    "best_impute_method = study.best_params.get('imputation', 'mean')\n",
    "\n",
    "# Apply the best imputation method\n",
    "if best_impute_method == 'mean':\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "elif best_impute_method == 'median':\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "elif best_impute_method == 'knn':\n",
    "    imputer = KNNImputer(n_neighbors=study.best_params.get('knn_neighbors', 5))\n",
    "elif best_impute_method == 'iterative':\n",
    "    imputer = IterativeImputer(\n",
    "        max_iter=study.best_params.get('iterative_max_iter', 50),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Fit and transform the data\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame (if needed)\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Now apply your feature selection code from before\n",
    "best_fs_method = study.best_params.get('feature_selection', 'None')\n",
    "\n",
    "if best_fs_method != 'None':\n",
    "    k_features = study.best_params['k_features']\n",
    "    \n",
    "    if best_fs_method == 'ANOVA':\n",
    "        selector = SelectKBest(f_regression, k=k_features)\n",
    "    elif best_fs_method == 'MutualInfo':\n",
    "        selector = SelectKBest(mutual_info_regression, k=k_features)\n",
    "    elif best_fs_method == 'RFE':\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=study.best_params.get('rfe_n_estimators', 100),\n",
    "            max_depth=study.best_params.get('rfe_max_depth', 5),\n",
    "            random_state=42\n",
    "        )\n",
    "        selector = RFE(\n",
    "            estimator, \n",
    "            n_features_to_select=k_features,\n",
    "            step=study.best_params.get('rfe_step', 1)\n",
    "        )\n",
    "    elif best_fs_method == 'MRMR':\n",
    "        selector = MRMRTransformer(k_features=k_features)\n",
    "    \n",
    "    selector.fit(X_imputed, y)\n",
    "    if hasattr(selector, 'get_support'):  # For SelectKBest/RFE\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "    else:  # For MRMRTransformer\n",
    "        selected_features = selector.selected_features\n",
    "    X_best = X_imputed[selected_features]\n",
    "else:\n",
    "    X_best = X_imputed\n",
    "    selected_features = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = CatBoostRegressor(\n",
    "    iterations=study.best_params['iterations'],\n",
    "    depth=study.best_params['depth'],\n",
    "    learning_rate=study.best_params['learning_rate'],\n",
    "    l2_leaf_reg=study.best_params['l2_leaf_reg'],\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "\n",
    "# Train on full imputed data\n",
    "best_model.fit(X_best, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data with same transformations as training\n",
    "# 1. Apply the same imputation\n",
    "X_test_imputed = imputer.transform(X_test_scaled)  # Use the already fitted imputer\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "if isinstance(X_test_scaled, pd.DataFrame):\n",
    "    X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test_scaled.columns)\n",
    "\n",
    "# 2. Apply the same feature selection\n",
    "if best_fs_method != 'None':\n",
    "    if best_fs_method == 'MRMR':\n",
    "        X_test_final = X_test_imputed[selected_features]\n",
    "    else:\n",
    "        X_test_final = selector.transform(X_test_imputed)  # Use the already fitted selector\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test_final = pd.DataFrame(X_test_final, columns=selected_features)\n",
    "else:\n",
    "    X_test_final = X_test_imputed\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Calculate accuracy within ±1 point\n",
    "correct = np.sum(np.abs(y_test - y_pred) <= 1)\n",
    "accuracy = correct / len(y_test)\n",
    "print(f\"Accuracy within ±1 point: {accuracy:.4f}\")\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, label='Predictions')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', label='Perfect prediction')\n",
    "plt.xlabel('Actual Comfort Score')\n",
    "plt.ylabel('Predicted Comfort Score')\n",
    "plt.title(f'Test Set Performance\\n({best_model_name} with {best_fs_method} feature selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Feature importance if available\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if hasattr(selector, 'get_support'):\n",
    "        features = X.columns[selector.get_support()]\n",
    "    else:\n",
    "        features = selected_features\n",
    "    importances = pd.Series(best_model.feature_importances_, index=features)\n",
    "    importances.sort_values().plot(kind='barh')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bessy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
