{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e123518",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b04e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "import optuna\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import mrmr\n",
    "from mrmr import mrmr_classif, mrmr_regression\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf37b46",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650bb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "file_path = \"features-Master.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Shuffle the data\n",
    "shuffled = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_shuffled = shuffled.iloc[:, 4:]\n",
    "labels_shuffled = shuffled[\"Comfort Score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30f59a",
   "metadata": {},
   "source": [
    "# Break Data into Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7556be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For regression (using stratified split based on binned target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_shuffled, \n",
    "    labels_shuffled, \n",
    "    test_size=0.1, \n",
    "    stratify=labels_shuffled,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd2936",
   "metadata": {},
   "source": [
    "# Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c934c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a0af2",
   "metadata": {},
   "source": [
    "# MRMR Conversion Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e96953",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRMRTransformer:\n",
    "    def __init__(self, k_features):\n",
    "        self.k_features = k_features\n",
    "        self.selected_features = None\n",
    "        self.column_names = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Convert to DataFrame if not already\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        # Reset indices to avoid alignment issues\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "        \n",
    "        self.column_names = X.columns.tolist()\n",
    "        try:\n",
    "            self.selected_features = mrmr_regression(X, y, K=self.k_features)\n",
    "        except:\n",
    "            # Fallback to random features if MRMR fails\n",
    "            self.selected_features = np.random.choice(X.columns, size=min(self.k_features, len(X.columns)), replace=False)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X, columns=self.column_names)\n",
    "        return X[self.selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e640d",
   "metadata": {},
   "source": [
    "# Regression Optimization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095e1bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train should be defined beforehand\n",
    "X = X_train_scaled.copy()\n",
    "y = y_train.copy().values  # Ensure numpy array for y\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Imputation\n",
    "    impute_method = trial.suggest_categorical('imputation', ['mean', 'median', 'knn', 'iterative'])\n",
    "    if impute_method == 'mean':\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "    elif impute_method == 'median':\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "    elif impute_method == 'knn':\n",
    "        imputer = KNNImputer(n_neighbors=trial.suggest_int('knn_neighbors', 3, 15))\n",
    "    else:\n",
    "        imputer = IterativeImputer(\n",
    "            max_iter=trial.suggest_int('iterative_max_iter', 10, 50),\n",
    "            random_state=42,\n",
    "            tol=0.01\n",
    "        )\n",
    "\n",
    "    # 2. Feature Selection\n",
    "    fs_method = trial.suggest_categorical('feature_selection', ['ANOVA', 'MutualInfo', 'RFE', 'MRMR', 'None'])\n",
    "    if fs_method != 'None':\n",
    "        k_features = trial.suggest_int('k_features', 10, min(50, X.shape[1]))\n",
    "        if fs_method == 'ANOVA':\n",
    "            selector = SelectKBest(f_regression, k=k_features)\n",
    "        elif fs_method == 'MutualInfo':\n",
    "            selector = SelectKBest(mutual_info_regression, k=k_features)\n",
    "        elif fs_method == 'RFE':\n",
    "            rfe_step = trial.suggest_float('rfe_step', 0.1, 1.0)\n",
    "            estimator = RandomForestRegressor(\n",
    "                n_estimators=trial.suggest_int('rfe_n_estimators', 50, 200),\n",
    "                max_depth=trial.suggest_int('rfe_max_depth', 3, 10),\n",
    "                random_state=42\n",
    "            )\n",
    "            selector = RFE(estimator, n_features_to_select=k_features, step=rfe_step)\n",
    "        elif fs_method == 'MRMR':\n",
    "            selector = MRMRTransformer(k_features=k_features)\n",
    "    else:\n",
    "        selector = 'passthrough'\n",
    "\n",
    "    # 3. Model Selection\n",
    "    model_name = trial.suggest_categorical('model', ['RandomForest', 'GradientBoosting', 'CatBoost', 'XGBoost'])\n",
    "\n",
    "    if model_name == 'XGBoost':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = XGBRegressor(**params)\n",
    "    elif model_name == 'RandomForest':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = RandomForestRegressor(**params)\n",
    "    elif model_name == 'GradientBoosting':\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        model = GradientBoostingRegressor(**params)\n",
    "    else:  # CatBoost\n",
    "        params = {\n",
    "            'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "            'depth': trial.suggest_int('depth', 3, 10),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
    "            'random_state': 42,\n",
    "            'verbose': False\n",
    "        }\n",
    "        model = CatBoostRegressor(**params)\n",
    "\n",
    "    # 5. Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', imputer),\n",
    "        ('feature_selection', selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # 6. Cross-validation for regression\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2', n_jobs=1)\n",
    "        return np.mean(scores)\n",
    "    except Exception:\n",
    "        return -np.inf\n",
    "\n",
    "# Run Optuna Study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Best result\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"R²: {trial.value:.4f}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc09f5",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90fe86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# 1. Parallel Coordinate Plot (Best for seeing parameter relationships)\n",
    "fig = vis.plot_parallel_coordinate(\n",
    "    study,\n",
    "    params=[\n",
    "        'imputation',\n",
    "        'feature_selection',\n",
    "        'k_features',\n",
    "        'model',\n",
    "        'n_estimators',\n",
    "        'max_depth',\n",
    "        'learning_rate'\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 2. Parameter Importance Plot\n",
    "fig = vis.plot_param_importances(study, target_name=\"R² Score\")\n",
    "fig.show()\n",
    "\n",
    "# 3. Slice Plot (Best for seeing individual parameter effects)\n",
    "fig = vis.plot_slice(\n",
    "    study,\n",
    "    params=[\n",
    "        'n_estimators',\n",
    "        'max_depth',\n",
    "        'learning_rate',\n",
    "        'k_features'\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# 4. Contour Plot (For seeing parameter interactions)\n",
    "fig = vis.plot_contour(\n",
    "    study,\n",
    "    params=[\n",
    "        ('n_estimators', 'max_depth'),\n",
    "        ('learning_rate', 'max_depth'),\n",
    "    ],\n",
    "    target_name=\"R² Score\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3090b97f",
   "metadata": {},
   "source": [
    "# Apply best imputation and feature selection to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best imputation method from study\n",
    "best_impute_method = study.best_params.get('imputation', 'mean')\n",
    "\n",
    "# Apply the best imputation method\n",
    "if best_impute_method == 'mean':\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "elif best_impute_method == 'median':\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "elif best_impute_method == 'knn':\n",
    "    imputer = KNNImputer(n_neighbors=study.best_params.get('knn_neighbors', 5))\n",
    "elif best_impute_method == 'iterative':\n",
    "    imputer = IterativeImputer(\n",
    "        max_iter=study.best_params.get('iterative_max_iter', 50),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Fit and transform the data\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Convert back to DataFrame (if needed)\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X_imputed = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Now apply your feature selection code from before\n",
    "best_fs_method = study.best_params.get('feature_selection', 'None')\n",
    "\n",
    "if best_fs_method != 'None':\n",
    "    k_features = study.best_params['k_features']\n",
    "    \n",
    "    if best_fs_method == 'ANOVA':\n",
    "        selector = SelectKBest(f_regression, k=k_features)\n",
    "    elif best_fs_method == 'MutualInfo':\n",
    "        selector = SelectKBest(mutual_info_regression, k=k_features)\n",
    "    elif best_fs_method == 'RFE':\n",
    "        estimator = RandomForestRegressor(\n",
    "            n_estimators=study.best_params.get('rfe_n_estimators', 100),\n",
    "            max_depth=study.best_params.get('rfe_max_depth', 5),\n",
    "            random_state=42\n",
    "        )\n",
    "        selector = RFE(\n",
    "            estimator, \n",
    "            n_features_to_select=k_features,\n",
    "            step=study.best_params.get('rfe_step', 1)\n",
    "        )\n",
    "    elif best_fs_method == 'MRMR':\n",
    "        selector = MRMRTransformer(k_features=k_features)\n",
    "    \n",
    "    selector.fit(X_imputed, y)\n",
    "    if hasattr(selector, 'get_support'):  # For SelectKBest/RFE\n",
    "        selected_features = X.columns[selector.get_support()]\n",
    "    else:  # For MRMRTransformer\n",
    "        selected_features = selector.selected_features\n",
    "    X_best = X_imputed[selected_features]\n",
    "else:\n",
    "    X_best = X_imputed\n",
    "    selected_features = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74517f2",
   "metadata": {},
   "source": [
    "# Apply best model and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334693f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the best model\n",
    "best_model_name = study.best_params['model']\n",
    "\n",
    "if best_model_name == 'XGBoost':\n",
    "    best_model = XGBRegressor(\n",
    "        n_estimators=study.best_params['n_estimators'],\n",
    "        max_depth=study.best_params['max_depth'],\n",
    "        learning_rate=study.best_params['learning_rate'],\n",
    "        subsample=study.best_params['subsample'],\n",
    "        colsample_bytree=study.best_params['colsample_bytree'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif best_model_name == 'RandomForest':\n",
    "    best_model = RandomForestRegressor(\n",
    "        n_estimators=study.best_params['n_estimators'],\n",
    "        max_depth=study.best_params['max_depth'],\n",
    "        min_samples_split=study.best_params['min_samples_split'],\n",
    "        min_samples_leaf=study.best_params['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif best_model_name == 'GradientBoosting':\n",
    "    best_model = GradientBoostingRegressor(\n",
    "        n_estimators=study.best_params['n_estimators'],\n",
    "        max_depth=study.best_params['max_depth'],\n",
    "        learning_rate=study.best_params['learning_rate'],\n",
    "        min_samples_split=study.best_params['min_samples_split'],\n",
    "        min_samples_leaf=study.best_params['min_samples_leaf'],\n",
    "        subsample=study.best_params['subsample'],\n",
    "        random_state=42\n",
    "    )\n",
    "else: # best_model_name == 'CatBoost':\n",
    "    best_model = CatBoostRegressor(\n",
    "        iterations=study.best_params['iterations'],\n",
    "        depth=study.best_params['depth'],\n",
    "        learning_rate=study.best_params['learning_rate'],\n",
    "        l2_leaf_reg=study.best_params['l2_leaf_reg'],\n",
    "        random_state=42,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "# Train on full imputed data\n",
    "best_model.fit(X_best, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135f849",
   "metadata": {},
   "source": [
    "# Evaluate with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data with same transformations as training\n",
    "# 1. Apply the same imputation\n",
    "X_test_imputed = imputer.transform(X_test_scaled)  # Use the already fitted imputer\n",
    "\n",
    "# Convert back to DataFrame if needed\n",
    "if isinstance(X_test_scaled, pd.DataFrame):\n",
    "    X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test_scaled.columns)\n",
    "\n",
    "# 2. Apply the same feature selection\n",
    "if best_fs_method != 'None':\n",
    "    if best_fs_method == 'MRMR':\n",
    "        X_test_final = X_test_imputed[selected_features]\n",
    "    else:\n",
    "        X_test_final = selector.transform(X_test_imputed)  # Use the already fitted selector\n",
    "        if isinstance(X_test, pd.DataFrame):\n",
    "            X_test_final = pd.DataFrame(X_test_final, columns=selected_features)\n",
    "else:\n",
    "    X_test_final = X_test_imputed\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test_final)\n",
    "\n",
    "# Calculate metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nFinal Model Evaluation on Test Set:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# Calculate accuracy within ±1 point\n",
    "correct = np.sum(np.abs(y_test - y_pred) <= 1)\n",
    "accuracy = correct / len(y_test)\n",
    "print(f\"Accuracy within ±1 point: {accuracy:.4f}\")\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5, label='Predictions')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', label='Perfect prediction')\n",
    "plt.xlabel('Actual Comfort Score')\n",
    "plt.ylabel('Predicted Comfort Score')\n",
    "plt.title(f'Test Set Performance\\n({best_model_name} with {best_fs_method} feature selection)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Feature importance if available\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if hasattr(selector, 'get_support'):\n",
    "        features = X.columns[selector.get_support()]\n",
    "    else:\n",
    "        features = selected_features\n",
    "    importances = pd.Series(best_model.feature_importances_, index=features)\n",
    "    importances.sort_values().plot(kind='barh')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bessy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
