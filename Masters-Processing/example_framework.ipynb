{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline ML \n",
    "\n",
    "## Goals:\n",
    "\n",
    "1. **Data Import**\n",
    "    - Import EEG data (with associated markers).\n",
    "    - Import comfort bracket data.\n",
    "\n",
    "2. **Preprocessing**\n",
    "    - Preprocess the EEG data.\n",
    "    - Epoch data based on stimulus appearance.\n",
    "        - Tag each EEG epoch with the stimulus appearance and comfort score.\n",
    "\n",
    "    - Convert the bracket data to scores with Elo and export to excel sheet\n",
    "\n",
    "3. **Feature Extraction**\n",
    "    - Calculate the Power Spectral Density (PSD) for each epoch.\n",
    "    - Calculate the Z-score for each epoch:\n",
    "        * **Formula**:   PSD Z-score = (PSD(single trial) - Mean PSD of baseline trials) / Std PSD of baseline trials\n",
    "        \n",
    "4. **Data Formatting**\n",
    "    - Format data to feed it into the ML model.\n",
    "\n",
    "Want to make this as light as possible so only a few tweaks need to be made to run this in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import mne\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Import custom scripts\n",
    "from Functions import import_data\n",
    "from Functions import data_tools\n",
    "from Functions import elo\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "from mne.preprocessing import ICA\n",
    "\n",
    "\n",
    "# Enable interactive plots\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Import EEG data, should be a single file\n",
    "- Set channel names\n",
    "- Create an mne raw object (?)\n",
    "\n",
    "Import bracket data\n",
    "- Separate into matches (strings of stimulus names) and winners (string of stimulus name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=16, n_times=331520\n",
      "    Range : 0 ... 331519 =      0.000 ...  1294.996 secs\n",
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# import bracket data\n",
    "bracket_data = pd.read_csv(r'Data/Masters_testing/P001-S001.csv')\n",
    "matches = list(bracket_data[['Item 1', 'Item 2']].itertuples(index=False, name=None))\n",
    "winners = bracket_data['Winner'].tolist()\n",
    "\n",
    "# import EEG data\n",
    "file_name = \"Masters_testing/sub-P001_ses-S001_task-T1_run-001_eeg.xdf\"\n",
    "ch_names = [\"Fp1\", \"Af3\", \"Fz\", \"C3\", \"C4\", \"Cp1\", \"Cp4\", \"Tp8\", \"P5\", \"P1\", \"PO7\", \"POz\", \"PO8\", \"O1\", \"Oz\", \"O2\"]\n",
    "# Import data \n",
    "[eeg_ts, eeg_data, eeg_fs] = import_data.read_xdf(f\"Data\\\\{file_name}\", picks=ch_names)\n",
    "\n",
    "# Create MNE array\n",
    "info = mne.create_info(ch_names, eeg_fs, ch_types = 'eeg')  # Create info properties\n",
    "\n",
    "mne_raw = mne.io.RawArray(eeg_data, info = info)            # Create MNE raw array\n",
    "#mne_raw.set_montage('standard_1020')  \n",
    "\n",
    "#mne_raw.plot(title=\"RAW data\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandpass Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "hpf_fc = 0.5    # High-pass cut-off frequency: list[Hz]\n",
    "lpf_fc = 15\n",
    "\n",
    "# Apply high-pass filter\n",
    "filt_raw = mne_raw.copy().filter(l_freq=hpf_fc, h_freq=lpf_fc, picks=ch_names)\n",
    "filt_raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically Detect Bad Channels?\n",
    "ransac: https://autoreject.github.io/stable/auto_examples/plot_ransac.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch Data\n",
    "Want to epoch the EEG data based on stimulus labels\n",
    "\n",
    "This can look like \n",
    "1. [EEG]\n",
    "2. [Stimulus Labels]\n",
    "3. [Comfort Labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import markers\n",
    "[marker_ts, markers] = import_data.read_xdf_unity_markers(f\"Data\\{file_name}\")    # Import markers\n",
    "\n",
    "# Create epochs from Unity markerts\n",
    "(eeg_epochs, epoch_labels) = data_tools.epochs_from_unity_markers(\n",
    "    eeg_time = eeg_ts,\n",
    "    eeg_data = filt_raw.get_data(),\n",
    "    marker_time = marker_ts,\n",
    "    marker_data = markers\n",
    "    )\n",
    "\n",
    "# Get stimuli and frequencies labels\n",
    "dict_of_stimuli = data_tools.get_tvep_stimuli(epoch_labels) # Dictionary of unique stimulus\n",
    "dict_of_freqs = {0:\"10\"} \n",
    "\n",
    "# Create array of eeg epochs organized as [stimuli, freq, chans, samples]\n",
    "eeg_epochs_organized = data_tools.epochs_stim_freq(\n",
    "    eeg_epochs = eeg_epochs,\n",
    "    labels = epoch_labels,\n",
    "    stimuli = dict_of_stimuli,\n",
    "    freqs = dict_of_freqs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELO Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Elo Ranking object\n",
    "elo_system = elo.EloRanking()\n",
    "\n",
    "# Process matches and their winners\n",
    "for (competitor1, competitor2), winner in zip(matches, winners):\n",
    "    loser = competitor1 if winner == competitor2 else competitor2\n",
    "    elo_system.update_ratings(winner, loser)\n",
    "\n",
    "# Rankings before post-hoc adjustment\n",
    "print(\"Rankings before adjustment:\")\n",
    "print(pd.DataFrame(elo_system.get_rankings(), columns=[\"Stimulus\", \"Rating\"]))\n",
    "\n",
    "# Apply post-hoc adjustment\n",
    "elo_system.post_hoc_adjustment()\n",
    "\n",
    "# Rankings after post-hoc adjustment\n",
    "print(\"\\nRankings after adjustment:\")\n",
    "print(pd.DataFrame(elo_system.get_rankings(), columns=[\"Stimulus\", \"Rating\"]))\n",
    "\n",
    "# Scores after standardization from 1 - 10\n",
    "elo_system.standardize_rankings()\n",
    "print(\"\\n Standardized scores\")\n",
    "print(pd.DataFrame(elo_system.get_scores(), columns=[\"Stimulus\", \"Scores\"]))\n",
    "\n",
    "# Export the rankings to the data sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute PSD for all Epochs\n",
    "\n",
    "Make sure to include baseline epochs for later calculation of Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Z-score For Each Epoch\n",
    "This will create a fourth array [Z-scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove \"Off\" Epochs for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ML Model\n",
    "Choose several and test which has the best performance\n",
    "\n",
    "Decide on:\n",
    "- Train/test split\n",
    "- Which models to use\n",
    "    * https://scikit-learn.org/stable/auto_examples/neighbors/plot_regression.html#sphx-glr-auto-examples-neighbors-plot-regression-py\n",
    "    * https://scikit-learn.org/stable/modules/svm.html\n",
    "    * https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "- How to input all data\n",
    "- How to get the output I desire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Simulate EEG data (replace with actual data loading)\n",
    "# Example: 1000 samples, each with 128 time steps, and 8 EEG channels\n",
    "eeg_data = np.random.rand(1000, 128, 8)  # Shape: (samples, time steps, features)\n",
    "stimulus_rankings = np.random.randint(1, 6, size=(1000,))  # Comfort rankings (1 to 5)\n",
    "\n",
    "# Normalize EEG data\n",
    "scaler = StandardScaler()\n",
    "eeg_data = eeg_data.reshape(-1, eeg_data.shape[-1])  # Flatten for scaling\n",
    "eeg_data = scaler.fit_transform(eeg_data)\n",
    "eeg_data = eeg_data.reshape(-1, 128, 8)  # Reshape back\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, stimulus_rankings, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert rankings to one-hot if classification is needed\n",
    "y_train_one_hot = tf.keras.utils.to_categorical(y_train - 1, num_classes=5)\n",
    "y_val_one_hot = tf.keras.utils.to_categorical(y_val - 1, num_classes=5)\n",
    "y_test_one_hot = tf.keras.utils.to_categorical(y_test - 1, num_classes=5)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(128, 8), return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5, activation='softmax')  # Use softmax for classification, or change to a single neuron for regression\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',  # Use 'mse' for regression\n",
    "              metrics=['accuracy'])  # Use ['mae'] for regression\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_one_hot, \n",
    "                    validation_data=(X_val, y_val_one_hot),\n",
    "                    epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"eeg_lstm_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Info\n",
    "\n",
    "As other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of shape (n_samples, n_features) holding the training samples, \n",
    "and an array y of class labels (strings or integers), of shape (n_samples):\n",
    "\n",
    "Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1],\n",
    "or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results. This can be done easily by using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Simulate example dataset\n",
    "data = pd.DataFrame({\n",
    "    \"Epoch_Features\": [np.random.rand(32, 1280) for _ in range(24)],  # 32 channels, 1280 time points (sampling rate = 128 * 10 for 10 sec)\n",
    "    \"Size\": [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3], \n",
    "    \"Contrast\": [1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4],  \n",
    "    \"Comfort_Score\": [2, 3, 4, 7, 2, 4, 6, 7, 2, 4, 5, 7, 2, 3, 4, 6, 2, 4, 6, 7, 4, 4, 5, 7],  \n",
    "    \"Z_Score\": [2.1, 3, 2.2, 2.2, 2.2, 2.1, 1.5, 1, 3, 3, 4, 2.1, 2.1, 3, 2.2, 2.2, 2.2, 2.1, 1.5, 1, 3, 3, 4, 2.1]  \n",
    "})\n",
    "\n",
    "# Filter by Z-Score\n",
    "filtered_data = data[data['Z_Score'] > 2.0]\n",
    "\n",
    "# Flatten EEG features\n",
    "flattened_features = np.array([epoch.flatten() for epoch in filtered_data['Epoch_Features']])\n",
    "\n",
    "# Combine features\n",
    "X = np.hstack([\n",
    "    flattened_features,\n",
    "    filtered_data[['Size', 'Contrast']].values\n",
    "])\n",
    "y = filtered_data['Comfort_Score'].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split (to retain indices for Size/Contrast output)\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "    X_scaled, y, range(len(y)), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Get the corresponding Size and Contrast for the test set using test_indices\n",
    "test_data = filtered_data.iloc[test_indices]\n",
    "test_sizes = test_data['Size'].values\n",
    "test_contrasts = test_data['Contrast'].values\n",
    "\n",
    "# Define SVM model\n",
    "model = SVR(kernel='rbf', C=1.0, gamma='scale')\n",
    "\n",
    "# Cross-validation prediction\n",
    "y_pred_cv = cross_val_predict(model, X_scaled, y, cv=10)  # 10-fold cross-validation\n",
    "\n",
    "# Extract predictions only for the test set\n",
    "y_pred_test = y_pred_cv[test_indices]\n",
    "\n",
    "# Print out the Size, Contrast, and Prediction for each test epoch\n",
    "for i in range(len(y_pred_test)):\n",
    "    print(f\"Prediction: {y_pred_test[i]}, Size: {test_sizes[i]}, Contrast: {test_contrasts[i]}\")\n",
    "\n",
    "# Calculate RMSE for the cross-validation predictions on the test set\n",
    "rmse_cv = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(f\"RMSE (Cross-Validation): {rmse_cv}\")\n",
    "\n",
    "print(f\"Training epochs: {len(X_train)}\")\n",
    "print(f\"Testing epochs: {len(X_test)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
